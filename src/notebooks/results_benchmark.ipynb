{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the model likelihood of the same sentence under two models.\n",
    "\n",
    "> :warning: **Environment**: ipython has a lot of dependencies and is not in the main training environment. Hence this needs to run under a seperate one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from notebooks.utils import styles\n",
    "\n",
    "SAVE_BASE_PATH = \"/path/to/your/figures/benchmarks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load data from pickle files and store in DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the 8 digit ID of your saved experiment data\n",
    "results_likelihoods_names = [\"1234-abcd\"]\n",
    "relative_path_benchmark = \"pubmedqa/meta-llama_Meta-Llama-3-8B/results-categories-medical_qa-0-shot.pkl\"\n",
    "BENCHMARK_NAME = \"PubMedQA\" # start with \"_\"\n",
    "BENCHMARK_NAME_PRETTY = \"PubMedQA\"\n",
    "\n",
    "##### LOAD BY RUN NAMES\n",
    "run_names = results_likelihoods_names\n",
    "paths_likelihoods = [\n",
    "    f\"/path/to/your/artifacts/model_likelihood/{run_name}/model_likelihood.pkl\" for run_name in run_names\n",
    "]\n",
    "\n",
    "paths_benchmarks = [os.path.join(\"/path/to/your/artifacts/Domain-Certification/benchmarks/\", relative_path_benchmark) for _ in run_names]\n",
    "\n",
    "print(paths_likelihoods)\n",
    "print(f\"Loaded {len(paths_likelihoods)} likelihood outputs.\")\n",
    "\n",
    "# ID_NAME = \"task_data_int_sort\"\n",
    "DOMAIN = \"MedicalQA\"\n",
    "ID_NAMES = [\"pubmedqa\", \"pubmedqa_generated\"]\n",
    "\n",
    "##### LOAD RESULTS FROM LIKELIHOODS\n",
    "# load table with $k$s for certificate. !!!! This table is generaated by the notebook `results_likelihoods.ipynb`\n",
    "ood_bound_table_path = \"/path/to/your/code/src/notebooks/k_given_eps_table_<DATASET>_DC_quantile_1.00.csv\"\n",
    "\n",
    "\n",
    "##### PROCESS RESULTS\n",
    "ood_bound_table = pd.read_csv(ood_bound_table_path, sep=\"\\t\")\n",
    "ood_bound_table[\"k\"] = ood_bound_table[\"k_mid\"]\n",
    "print(f\"Loaded bound tabl for log10 epsilons from min={ood_bound_table['log10_eps'].min()} to max={ood_bound_table['log10_eps'].max()}\")\n",
    "\n",
    "benchmark_chance_performance = {\"MMLU\": 0.25, \"PubMedQA\": 0.33}[BENCHMARK_NAME]\n",
    "\n",
    "assert len(paths_likelihoods) == len(paths_benchmarks)\n",
    "\n",
    "SAVE_BASE_PATH = os.path.join(SAVE_BASE_PATH, DOMAIN, BENCHMARK_NAME)\n",
    "os.makedirs(SAVE_BASE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "for path_ll, path_benchmark in zip(paths_likelihoods, paths_benchmarks):\n",
    "    try:\n",
    "        with open(path_ll, 'rb') as f:\n",
    "            ll_results =pickle.load(f)\n",
    "        with open(path_benchmark, 'rb') as f:\n",
    "            benchmark_results = pickle.load(f)\n",
    "            experiments.append((ll_results, benchmark_results))\n",
    "    except:\n",
    "        print(f\"Could not load: {path_ll} or {path_benchmark}\")\n",
    "\n",
    "\n",
    "# compute the log likelihoods\n",
    "records = []\n",
    "for i, (experiment, benchmark_scores) in enumerate(experiments):\n",
    "    if \"x_text\" not in experiment[\"data\"]:\n",
    "        experiment[\"data\"][\"x_text\"] = experiment[\"data\"][\"x_text_generator\"]\n",
    "    if \"y_text\" not in experiment[\"data\"]:\n",
    "        experiment[\"data\"][\"y_text\"] = experiment[\"data\"][\"y_text_generator\"]\n",
    "    if \"n_token_prompt\" not in experiment[\"data\"]:\n",
    "        experiment[\"data\"][\"n_token_prompt\"] = experiment[\"data\"][\"n_token_prompt_generator\"]\n",
    "    if \"sequence_length\" not in experiment[\"data\"]:\n",
    "        experiment[\"data\"][\"sequence_length\"] = experiment[\"data\"][\"sequence_length_generator\"]\n",
    "    # check lengths\n",
    "    num_samples_ll = len(experiment[\"data\"][\"x_text\"])\n",
    "    num_samples_benchmark = len(benchmark_scores[\"results\"])\n",
    "    if num_samples_benchmark > num_samples_ll:\n",
    "        benchmark_scores[\"results\"] = benchmark_scores[\"results\"][:num_samples_ll]\n",
    "        num_samples_benchmark = len(benchmark_scores[\"results\"])\n",
    "    assert num_samples_ll == num_samples_benchmark\n",
    "\n",
    "    data = experiment['data']\n",
    "    N_samples = data[\"n_token_prompt\"].shape[0]\n",
    "    config = OmegaConf.to_container(experiment['config'])\n",
    "    ll_model = data[\"log_likelihoods_model\"].sum(-1)\n",
    "    ll_generator = data[\"log_likelihoods_generator\"].sum(-1)\n",
    "    ll2_model = ll_model / np.log(2)\n",
    "    ll2_generator = ll_generator / np.log(2)\n",
    "    ll10_model = ll_model / np.log(10)\n",
    "    ll10_generator = ll_generator / np.log(10)\n",
    "    n_token_response =  data[\"sequence_length\"] - data[\"n_token_prompt\"].squeeze()\n",
    "\n",
    "    log2_ratio = ll2_model - ll2_generator\n",
    "    norm_log2_ratio = log2_ratio / n_token_response\n",
    "\n",
    "    benchmark_correct = [int(x[\"correct\"]) for x in benchmark_scores[\"results\"]]\n",
    "    subject = [x[\"subject\"] for x in benchmark_scores[\"results\"]]\n",
    "\n",
    "    # get response string length\n",
    "    n_char_correct_response_benchmark = [len(row[\"choices\"][row[\"correct_answer_id\"]]) for row in benchmark_scores[\"results\"]]\n",
    "    n_char_judged_response_benchmark = [len(x) for x in data[\"y_text\"]]\n",
    "\n",
    "    config_columns = pd.json_normalize(config)\n",
    "    dist_F = config_columns['model.target_distribution'][0]\n",
    "    dist_G = config_columns['generator.target_distribution'][0]\n",
    "\n",
    "    entropy_model = data[\"entropy_model\"].sum(-1)\n",
    "    entropy_generator = data[\"entropy_generator\"].sum(-1)\n",
    "\n",
    "    config_columns[\"distributions\"] = f\"F({dist_F})||G({dist_G})\"\n",
    "\n",
    "    if config[\"inference\"][\"prompt_length\"] == \"dataset\":\n",
    "        prompt_length = data[\"n_token_prompt\"].squeeze()\n",
    "    else:\n",
    "        prompt_length = np.full((N_samples,), int(config[\"inference\"][\"prompt_length\"]))\n",
    "\n",
    "\n",
    "    data_columns = pd.DataFrame({\n",
    "        \"ll2_model\": ll2_model,\n",
    "        \"ll2_generator\": ll2_generator,\n",
    "        \"ll10_model\": ll10_model,\n",
    "        \"ll10_generator\": ll10_generator,\n",
    "        \"ll2_model_norm\": ll2_model / n_token_response,\n",
    "        \"ll2_generator_norm\": ll2_generator / n_token_response,\n",
    "        \"ll10_model_norm\": ll10_model / n_token_response,\n",
    "        \"ll10_generator_norm\": ll10_generator / n_token_response,\n",
    "        \"log2_ratio\": log2_ratio,\n",
    "        \"log2_ratio_norm\": log2_ratio / n_token_response,\n",
    "        \"entropy_model\": entropy_model,\n",
    "        \"entropy_generator\": entropy_generator,\n",
    "        \"x\": data[\"x_text\"],\n",
    "        \"y\": data[\"y_text\"],\n",
    "        \"sequence_length\": data[\"sequence_length\"],\n",
    "        \"n_token_prompt\": data[\"n_token_prompt\"].squeeze(),\n",
    "        \"prompt_length\": prompt_length,\n",
    "        \"n_token_response\": n_token_response,\n",
    "        \"benchmark_correct\": benchmark_correct,\n",
    "        \"n_char_correct_response_benchmark\": n_char_correct_response_benchmark,\n",
    "        \"n_char_judged_response_benchmark\": n_char_judged_response_benchmark,\n",
    "        \"category\": subject,\n",
    "    })\n",
    "\n",
    "    assert data_columns[\"n_token_response\"].min() > 0\n",
    "\n",
    "    # combine and repeat config_columns to match data_columns\n",
    "    config_columns = pd.concat([config_columns] * len(data_columns), ignore_index=True)\n",
    "    combined = pd.concat([config_columns, data_columns], axis=1)\n",
    "\n",
    "    records.append(combined)\n",
    "\n",
    "df = pd.concat(records).copy()\n",
    "\n",
    "# OOD is when data_config_name is not task_data_int_sort\n",
    "df[\"OOD\"] = (~df[\"data_config_name\"].isin(ID_NAMES)).astype(int)\n",
    "df[\"Dataset\"] = df[\"OOD\"].map({0: r\"Target Domain $\\mathcal{D}_{T}$\", 1: r\"Other $\\mathcal{D}_{F}$\"})\n",
    "\n",
    "# clean performance\n",
    "CLEAN_PERFORMANCE = df[\"benchmark_correct\"].mean().item()\n",
    "print(f\"{BENCHMARK_NAME} clean performance: {CLEAN_PERFORMANCE:.2%}\")\n",
    "\n",
    "# print model\n",
    "print(f\"Model:       {df['model.name_or_path'].unique()[0]}\")\n",
    "print(f\"Generator:   {df['generator.name_or_path'].unique()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "all_run_names = \"_\".join(run_names)\n",
    "save_path = os.path.join(SAVE_BASE_PATH, f\"df_{all_run_names}.csv\")\n",
    "df.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_upper_bound_m(log_g: float, N: int, T: int, k: float) -> float:\n",
    "    return log_g + np.log2(T) + k * N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRR vs Epsilon-Certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for old files with outdated naming\n",
    "if \"log10_cr_10\" not in ood_bound_table.columns:\n",
    "    ood_bound_table[\"log10_cr_10\"] = ood_bound_table[\"cr_10\"]\n",
    "    ood_bound_table[\"log10_cr_median\"] = ood_bound_table[\"cr_median\"]\n",
    "    ood_bound_table[\"log10_cr_90\"] = ood_bound_table[\"cr_90\"]\n",
    "\n",
    "# convert ood_bound_table to dict: log10_eps -> k\n",
    "ood_bound_dict = dict(zip(ood_bound_table[\"log10_eps\"], ood_bound_table[\"k\"]))\n",
    "\n",
    "ood_k_cr_dict = {row[\"k\"].item(): {\n",
    "    \"log10_cr_10\": row[\"log10_cr_10\"].item(),\n",
    "    \"log10_cr_median\": row[\"log10_cr_median\"].item(),\n",
    "    \"log10_cr_90\": row[\"log10_cr_90\"].item()\n",
    "    } for _, row in ood_bound_table.iterrows()}\n",
    "\n",
    "\n",
    "metrics = []\n",
    "\n",
    "df_ood = df.loc[df[\"OOD\"] == 1]\n",
    "\n",
    "for e_log, k in ood_bound_dict.items():\n",
    "    e_log2 = e_log / np.log10(2)\n",
    "    preds = df['log2_ratio_norm'] > k\n",
    "    sample_accepted = ~preds\n",
    "    benchmark_score = df[\"benchmark_correct\"].astype(bool)\n",
    "    correct_at_k = (benchmark_score & sample_accepted)\n",
    "\n",
    "    correct_at_k_per_category = {}\n",
    "    for category in df[\"category\"].unique():\n",
    "        correct_at_k_per_category[category] = correct_at_k[df[\"category\"] == category].mean()\n",
    "\n",
    "    cr_values = ood_k_cr_dict[k]\n",
    "\n",
    "    frr = preds.mean() # in the benchmark test, we only have ID samples\n",
    "    accuracy_at_k = correct_at_k.mean()\n",
    "\n",
    "    metrics.append({\n",
    "        \"e_log\": e_log,\n",
    "        \"FRR\": frr,\n",
    "        BENCHMARK_NAME: accuracy_at_k,\n",
    "        \"k\": k,\n",
    "        \"log10_cr_median\": cr_values[\"log10_cr_median\"],\n",
    "        \"log10_cr_10\": cr_values[\"log10_cr_10\"],\n",
    "        \"log10_cr_90\": cr_values[\"log10_cr_90\"],\n",
    "        **{f\"{BENCHMARK_NAME}_{category}\": c for category, c in correct_at_k_per_category.items()}\n",
    "    })\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "\n",
    "metrics_long = pd.melt(metrics, id_vars=[\"e_log\", \"log10_cr_median\"], value_vars=[\"FRR\", BENCHMARK_NAME], var_name=\"Metric\", value_name=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = metrics_long.loc[(metrics_long[\"e_log\"] >= -20) & (metrics_long[\"e_log\"] <= 2) & (metrics_long[\"Metric\"] == BENCHMARK_NAME)] # subset for plotting the most interestin range\n",
    "metrics_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update(styles.third)\n",
    "\n",
    "# plt.figure(figsize=(8, 3.25))\n",
    "x_axis = \"e_log\"\n",
    "g = sns.lineplot(data=metrics, x=x_axis, y=\"FRR\", label=\"FRR\")\n",
    "g = sns.lineplot(data=metrics, x=x_axis, y=BENCHMARK_NAME, label=rf\"{BENCHMARK_NAME_PRETTY}@$\\epsilon$\")\n",
    "g.set(ylabel=\"\")\n",
    "g.set(yticks=[0, 0.25, 0.5, 0.75, 1])\n",
    "g.set(xlabel=r\"$\\text{log}_{10}$ $\\epsilon$-DC\")\n",
    "\n",
    "# PubMedQA\n",
    "g.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.25), ncol=2, frameon=False, columnspacing=0.5)\n",
    "g.set(xlim=(-20, 2))\n",
    "g.set(xticks=[-20, -15, -10, -5, 0])\n",
    "\n",
    "save_path = os.path.join(SAVE_BASE_PATH, f\"benchmark_epsilon_dc_{DOMAIN}_{BENCHMARK_NAME}.pdf\")\n",
    "plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "print(f\"Saved figure to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update(styles.third)\n",
    "\n",
    "x_axis = \"log10_cr_median\"\n",
    "g = sns.lineplot(data=metrics, x=x_axis, y=\"FRR\", label=\"FRR\")\n",
    "g = sns.lineplot(data=metrics, x=x_axis, y=BENCHMARK_NAME, label=rf\"{BENCHMARK_NAME_PRETTY}@$\\epsilon$\")\n",
    "g.set(ylabel=\"\")\n",
    "g.set(xlabel=r\"$\\log_{10} CR_k$ (Median)\")\n",
    "g.set(yticks=[0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "# g.legend(loc='lower left', bbox_to_anchor=(0.0, 0.2))\n",
    "g.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.25), ncol=2, frameon=False, columnspacing=0.5)\n",
    "g.set(xlim=(-1, 42))\n",
    "\n",
    "save_path = os.path.join(SAVE_BASE_PATH, f\"benchmark_cr_{DOMAIN}_{BENCHMARK_NAME}.pdf\")\n",
    "# plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "print(f\"Saved figure to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interactive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
